{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub\n",
    "import cv2\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_name = 'dataset/asl_alphabet_train'\n",
    "trn_dirs = [os.path.join(trn_name, sign) for sign in os.listdir(trn_name)]\n",
    "trn_fnames = [os.listdir(sign_img) for sign_img in trn_dirs]\n",
    "\n",
    "tst_name = 'dataset/asl_alphabet_test'\n",
    "tst_dirs = [os.path.join(tst_name, sign) for sign in os.listdir(tst_name)]\n",
    "tst_fnames = [os.listdir(sign_img) for sign_img in tst_dirs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 69600 images belonging to 29 classes.\n",
      "Found 17400 images belonging to 29 classes.\n",
      "Found 29 images belonging to 29 classes.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "\n",
    "BATCH_SIZE = 100\n",
    "IMAGE_SHAPE = (224, 224)\n",
    "\n",
    "\n",
    "image_generator = tf.keras.preprocessing.image.ImageDataGenerator(validation_split=0.2,\n",
    "    rescale=1. / 255)\n",
    "\n",
    "\n",
    "train_generator = image_generator.flow_from_directory(\n",
    "    trn_name,\n",
    "    subset='training',\n",
    "    batch_size=BATCH_SIZE,\n",
    "    target_size=IMAGE_SHAPE,\n",
    "    class_mode=\"categorical\")\n",
    "\n",
    "vldn_generator = image_generator.flow_from_directory(\n",
    "    trn_name,\n",
    "    subset='validation',\n",
    "    batch_size=BATCH_SIZE,\n",
    "    target_size=IMAGE_SHAPE,\n",
    "    class_mode=\"categorical\")\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "tst_generator = image_generator.flow_from_directory(\n",
    "    tst_name,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    target_size=IMAGE_SHAPE,\n",
    "    class_mode=\"categorical\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = train_generator.samples\n",
    "vldn_size = vldn_generator.samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " keras_layer_2 (KerasLayer)  (None, 1280)              11837936  \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 1280)              0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 290)               371490    \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 290)               0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 29)                8439      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 12,217,865\n",
      "Trainable params: 379,929\n",
      "Non-trainable params: 11,837,936\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_url = \"https://tfhub.dev/tensorflow/efficientnet/lite4/feature-vector/2\"\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.InputLayer(input_shape=IMAGE_SHAPE + (3,)),\n",
    "    tensorflow_hub.KerasLayer(model_url, trainable=False),\n",
    "    tf.keras.layers.Dropout(rate=0.2),\n",
    "    tf.keras.layers.Dense(290, activation='relu'),\n",
    "    tf.keras.layers.Dropout(rate=0.2),\n",
    "    tf.keras.layers.Dense(29, activation='softmax',\n",
    "                          kernel_regularizer=tf.keras.regularizers.l2(0.0001))\n",
    "])\n",
    "\n",
    "model.build((None,) + IMAGE_SHAPE + (3,))\n",
    "model.summary()\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.SGD(learning_rate=0.005, momentum=0.9),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['acc'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./checkpoints/cp.ckpt\n"
     ]
    }
   ],
   "source": [
    "checkpoint_path = \"./checkpoints/cp.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "print(checkpoint_path)\n",
    "\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_path,\n",
    "    verbose=1,\n",
    "    save_weights_only=True,\n",
    "    save_freq=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.momentum\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.momentum\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 19/696 [..............................] - ETA: 13:30 - loss: 3.4224 - acc: 0.0679\n",
      "Epoch 00001: saving model to ./checkpoints\\cp.ckpt\n",
      " 39/696 [>.............................] - ETA: 13:21 - loss: 3.1173 - acc: 0.1508\n",
      "Epoch 00001: saving model to ./checkpoints\\cp.ckpt\n",
      " 59/696 [=>............................] - ETA: 13:00 - loss: 2.8757 - acc: 0.2247\n",
      "Epoch 00001: saving model to ./checkpoints\\cp.ckpt\n",
      " 79/696 [==>...........................] - ETA: 12:37 - loss: 2.6447 - acc: 0.2859\n",
      "Epoch 00001: saving model to ./checkpoints\\cp.ckpt\n",
      " 99/696 [===>..........................] - ETA: 12:14 - loss: 2.4484 - acc: 0.3380\n",
      "Epoch 00001: saving model to ./checkpoints\\cp.ckpt\n",
      "119/696 [====>.........................] - ETA: 11:50 - loss: 2.2767 - acc: 0.3852\n",
      "Epoch 00001: saving model to ./checkpoints\\cp.ckpt\n",
      "139/696 [====>.........................] - ETA: 11:25 - loss: 2.1330 - acc: 0.4217\n",
      "Epoch 00001: saving model to ./checkpoints\\cp.ckpt\n",
      "159/696 [=====>........................] - ETA: 11:01 - loss: 2.0147 - acc: 0.4520\n",
      "Epoch 00001: saving model to ./checkpoints\\cp.ckpt\n",
      "179/696 [======>.......................] - ETA: 10:36 - loss: 1.9069 - acc: 0.4787\n",
      "Epoch 00001: saving model to ./checkpoints\\cp.ckpt\n",
      "199/696 [=======>......................] - ETA: 10:11 - loss: 1.8117 - acc: 0.5040\n",
      "Epoch 00001: saving model to ./checkpoints\\cp.ckpt\n",
      "219/696 [========>.....................] - ETA: 9:47 - loss: 1.7280 - acc: 0.5266\n",
      "Epoch 00001: saving model to ./checkpoints\\cp.ckpt\n",
      "239/696 [=========>....................] - ETA: 9:22 - loss: 1.6543 - acc: 0.5464\n",
      "Epoch 00001: saving model to ./checkpoints\\cp.ckpt\n",
      "259/696 [==========>...................] - ETA: 8:57 - loss: 1.5891 - acc: 0.5638\n",
      "Epoch 00001: saving model to ./checkpoints\\cp.ckpt\n",
      "279/696 [===========>..................] - ETA: 8:33 - loss: 1.5274 - acc: 0.5805\n",
      "Epoch 00001: saving model to ./checkpoints\\cp.ckpt\n",
      "299/696 [===========>..................] - ETA: 8:08 - loss: 1.4755 - acc: 0.5941\n",
      "Epoch 00001: saving model to ./checkpoints\\cp.ckpt\n",
      "319/696 [============>.................] - ETA: 7:43 - loss: 1.4271 - acc: 0.6069\n",
      "Epoch 00001: saving model to ./checkpoints\\cp.ckpt\n",
      "339/696 [=============>................] - ETA: 7:19 - loss: 1.3818 - acc: 0.6188\n",
      "Epoch 00001: saving model to ./checkpoints\\cp.ckpt\n",
      "359/696 [==============>...............] - ETA: 6:54 - loss: 1.3418 - acc: 0.6292\n",
      "Epoch 00001: saving model to ./checkpoints\\cp.ckpt\n",
      "379/696 [===============>..............] - ETA: 6:29 - loss: 1.3046 - acc: 0.6396\n",
      "Epoch 00001: saving model to ./checkpoints\\cp.ckpt\n",
      "399/696 [================>.............] - ETA: 6:05 - loss: 1.2699 - acc: 0.6489\n",
      "Epoch 00001: saving model to ./checkpoints\\cp.ckpt\n",
      "419/696 [=================>............] - ETA: 5:40 - loss: 1.2376 - acc: 0.6577\n",
      "Epoch 00001: saving model to ./checkpoints\\cp.ckpt\n",
      "439/696 [=================>............] - ETA: 5:16 - loss: 1.2060 - acc: 0.6663\n",
      "Epoch 00001: saving model to ./checkpoints\\cp.ckpt\n",
      "459/696 [==================>...........] - ETA: 4:51 - loss: 1.1778 - acc: 0.6739\n",
      "Epoch 00001: saving model to ./checkpoints\\cp.ckpt\n",
      "479/696 [===================>..........] - ETA: 4:26 - loss: 1.1517 - acc: 0.6810\n",
      "Epoch 00001: saving model to ./checkpoints\\cp.ckpt\n",
      "499/696 [====================>.........] - ETA: 4:02 - loss: 1.1272 - acc: 0.6876\n",
      "Epoch 00001: saving model to ./checkpoints\\cp.ckpt\n",
      "519/696 [=====================>........] - ETA: 3:37 - loss: 1.1029 - acc: 0.6942\n",
      "Epoch 00001: saving model to ./checkpoints\\cp.ckpt\n",
      "539/696 [======================>.......] - ETA: 3:13 - loss: 1.0801 - acc: 0.7003\n",
      "Epoch 00001: saving model to ./checkpoints\\cp.ckpt\n",
      "559/696 [=======================>......] - ETA: 2:48 - loss: 1.0587 - acc: 0.7062\n",
      "Epoch 00001: saving model to ./checkpoints\\cp.ckpt\n",
      "579/696 [=======================>......] - ETA: 2:23 - loss: 1.0387 - acc: 0.7115\n",
      "Epoch 00001: saving model to ./checkpoints\\cp.ckpt\n",
      "599/696 [========================>.....] - ETA: 1:59 - loss: 1.0188 - acc: 0.7169\n",
      "Epoch 00001: saving model to ./checkpoints\\cp.ckpt\n",
      "619/696 [=========================>....] - ETA: 1:34 - loss: 1.0008 - acc: 0.7217\n",
      "Epoch 00001: saving model to ./checkpoints\\cp.ckpt\n",
      "639/696 [==========================>...] - ETA: 1:10 - loss: 0.9825 - acc: 0.7267\n",
      "Epoch 00001: saving model to ./checkpoints\\cp.ckpt\n",
      "659/696 [===========================>..] - ETA: 45s - loss: 0.9657 - acc: 0.7311\n",
      "Epoch 00001: saving model to ./checkpoints\\cp.ckpt\n",
      "679/696 [============================>.] - ETA: 20s - loss: 0.9496 - acc: 0.7357\n",
      "Epoch 00001: saving model to ./checkpoints\\cp.ckpt\n",
      "696/696 [==============================] - 874s 1s/step - loss: 0.9373 - acc: 0.7389 - val_loss: 0.6406 - val_acc: 0.8270\n",
      "INFO:tensorflow:Assets written to: ./model_saves\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model_saves\\assets\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "if os.path.exists('./checkpoints') == True:\n",
    "  latest_checkpoint = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "  model.load_weights(checkpoint_path)\n",
    "\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    validation_data=vldn_generator,\n",
    "    epochs=1,\n",
    "    validation_steps=10,\n",
    "    verbose=1,\n",
    "    callbacks=[cp_callback])\n",
    "\n",
    "model.save('./model_saves')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B',\n",
       " 'S',\n",
       " 'C',\n",
       " 'D',\n",
       " 'X',\n",
       " 'N',\n",
       " 'E',\n",
       " 'nothing',\n",
       " 'G',\n",
       " 'nothing',\n",
       " 'W',\n",
       " 'space',\n",
       " 'X',\n",
       " 'R',\n",
       " 'G',\n",
       " 'O',\n",
       " 'N',\n",
       " 'nothing',\n",
       " 'O',\n",
       " 'D',\n",
       " 'Q',\n",
       " 'X',\n",
       " 'R',\n",
       " 'K',\n",
       " 'K',\n",
       " 'space',\n",
       " 'W',\n",
       " 'V',\n",
       " 'nothing']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "test_imgs = [\n",
    "    'dataset/asl_alphabet_train/X/X2.jpg'\n",
    "]\n",
    "\n",
    "test_imgs = [os.path.join(file, os.listdir(file)[0]) for file in tst_dirs]\n",
    "\n",
    "imgs = [np.expand_dims(\n",
    "    image.img_to_array(\n",
    "        image.load_img(img, target_size=IMAGE_SHAPE)\n",
    "    ), axis=0)\n",
    "    for img in test_imgs]\n",
    "\n",
    "# pass the list of multiple images np.vstack()\n",
    "images = np.vstack(imgs)\n",
    "classes = model.predict(images, batch_size=10)\n",
    "# print the classes, the images belong to\n",
    "all_classes = os.listdir(trn_name)\n",
    "final_results = []\n",
    "for probablities in classes:\n",
    "    final_results.append(all_classes[np.argmax(probablities)]) \n",
    "print(final_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E\n"
     ]
    }
   ],
   "source": [
    "# Live Detection\n",
    "def predict_asl(frame) -> str:\n",
    "\tframe = image.smart_resize(frame, (224, 224))\n",
    "\timg = np.expand_dims(image.img_to_array(frame), axis=0)\n",
    "\tpredicted_asl = model.predict(img)\n",
    "\treturn all_classes[np.argmax(predicted_asl)]\n",
    "\n",
    "alphabet = ''\n",
    "cap = cv2.VideoCapture(0)\n",
    "frame_number = 0\n",
    "while(True):\n",
    "\tret, frame = cap.read()\n",
    "\tcv2.putText(frame,\n",
    "             alphabet,\n",
    "             (50, 50),\n",
    "             font, 1,\n",
    "             (0, 255, 255),\n",
    "             2,\n",
    "             cv2.LINE_4)\n",
    "\tcv2.imshow(\"Webcam\", frame)\n",
    "\twhile (frame_number == 180):\n",
    "\t\talphabet = predict_asl(frame)\n",
    "\t\tprint(alphabet)\n",
    "\t\tfont = cv2.FONT_HERSHEY_SIMPLEX\n",
    "\t\tframe_number = 0\n",
    "\tframe_number += 1\n",
    "\n",
    "\n",
    "\tch = cv2.waitKey(1)\n",
    "\tif ch & 0xFF == ord('q'):\n",
    "\t\tcap.release()\n",
    "\t\tbreak\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7c4337cb28130a45e22977257afd0fd0a7cba64c2a30280441f9418d3d63af4d"
  },
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
